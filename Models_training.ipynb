{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,ElasticNet,Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, KFold,RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create handlers\n",
    "f_handler = logging.FileHandler('ModelTraining.log')\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "f_handler.setFormatter(f_format)\n",
    "# Set level of logging\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(f_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    '''\n",
    "    class FeatureEngineering walk on the row data to re arrange, handle missing data \n",
    "    and deal with outliers \n",
    "    '''\n",
    "    @staticmethod\n",
    "    def rearrange_features(df):\n",
    "        '''\n",
    "        this function to re format data \n",
    "        some values are not acceptable\n",
    "        some work of this function done in the data collector also, \n",
    "        when re formatiing number of bedrooms, baths, and re arrange amenities columns\n",
    "        '''\n",
    "        #drop all columns contain Unnamed in thier names\n",
    "        df = df.iloc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        # update the values in series of house price in df by replacing K\n",
    "        df.update(df[df['house_price'].str.contains('K')].iloc[:,0].str.replace('K','000'))\n",
    "        # after arranging amenities as columns, drop am column which contanins all amenities \n",
    "        df = df.drop(columns=['am'])\n",
    "        # change the type of values in the df\n",
    "        df=df.astype('int')\n",
    "        logger.info('data has been arranged in suitable format')\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def na_percentage_in_rows(df):\n",
    "        '''\n",
    "        This function search for missing values in Rows, \n",
    "        return a stat of all rows with missing values, \n",
    "        each row with its missing values percentage.  \n",
    "        '''\n",
    "        # get all index of rows that contain NaNs \n",
    "        is_NaN = df. isnull()\n",
    "        row_has_NaN = is_NaN. any(axis=1)\n",
    "        rows_with_NaN = df[row_has_NaN]\n",
    "\n",
    "        # create stat with rows index and its NaN value percentage\n",
    "        stat = pd.DataFrame()\n",
    "        stat['row'] = df[row_has_NaN].index\n",
    "        stat['na percentage'] = ((36-df[row_has_NaN].apply(lambda x: x.count(), axis=1))/36).tolist()\n",
    "        # return all index has a percentage of missing more than or equal 0.5 \n",
    "        logger.info('Index of rows which have missings values : {}'.format(df[row_has_NaN].index))\n",
    "        return stat,stat[stat['na percentage']>=0.5].iloc[:,0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def na_percentage_in_cols(df):\n",
    "        '''\n",
    "        This function search for missing values in features, \n",
    "        return a stat of all columns with missing values, \n",
    "        each column with its missing values percentage. \n",
    "        '''\n",
    "        # create stat with columns and its NaN value percentage\n",
    "        stat =pd.DataFrame()\n",
    "        stat['col'] = df.columns\n",
    "        stat['na percentage']=df.isna().mean().tolist()\n",
    "        logger.info('features which have missings values : {}'.format(stat[stat['na percentage']>=0.5].iloc[:,0]))\n",
    "        # return all columns name has a percentage of missing more than or equal 0.5 \n",
    "        return stat,stat[stat['na percentage']>=0.5].iloc[:,0]\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_missings(df):\n",
    "        '''\n",
    "        This function check the percentage of NaN values in rows and columns \n",
    "        and deside wether to drop them or not\n",
    "        '''\n",
    "        s1,index=FeatureEngineering.na_percentage_in_rows(df)\n",
    "        s2,features=FeatureEngineering.na_percentage_in_cols(df)\n",
    "        print(features)\n",
    "        # drop all index has a percentage of missing more than or equal 0.5\n",
    "        df = df.drop(index)\n",
    "        if features.size!=0:\n",
    "            df = df.drop(columns = [features])\n",
    "        logger.info('Missing data has been handled in dataframe')\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def drop_outliers(df,data_series):\n",
    "        '''\n",
    "        This function deals with outliers \n",
    "        this function will get the data and get rid of all outliers using IQR analysis\n",
    "        '''\n",
    "        # calculate Q1,Q3\n",
    "        Q1, Q3 = df[data_series].quantile([0.25, 0.75]).values\n",
    "        # IQR value\n",
    "        IQR = Q3 - Q1\n",
    "        # find limits of the data  \n",
    "        lower_limit = Q1 - 1.5 * IQR\n",
    "        upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "        # finding index of all data outside the limits\n",
    "        s=df[data_series][( df[data_series] < lower_limit) |\n",
    "                          ( df[data_series] > upper_limit) ]\n",
    "        s.index\n",
    "        # drop data outside the limits\n",
    "        df = df.drop(s.index)\n",
    "        logger.info('Outliers Treatment')\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection:\n",
    "    '''\n",
    "    Class FeatureSelection get important features that has a high correlation \n",
    "    with target variable.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def select_features(df, target_variable):\n",
    "        '''\n",
    "        This function selects important features using SelectFromModel library \n",
    "        '''\n",
    "        # split data into train and test\n",
    "        x = df.drop(target_variable, 1)\n",
    "        y = df[target_variable]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=37)\n",
    "        \n",
    "        # random forest calssifier modelto use it in feature selection\n",
    "        model = rfc(n_estimators = 300, n_jobs = -1,random_state =37, min_samples_leaf = 50)\n",
    "        \n",
    "        # select from model with threashold 0.02, so select features with correlation more than 0.02 \n",
    "        sfm = SelectFromModel(model,threshold=0.02)\n",
    "        sfm.fit(x_train, y_train)\n",
    "        # get columns name of important features\n",
    "        selected_features = x_train.columns[(sfm.get_support())]\n",
    "        \n",
    "        # Creating a bar plot to show the selected features correlation and importance\n",
    "        font = {'size'   : 7}\n",
    "        matplotlib.rc('font', **font)\n",
    "        model.fit(x_train, y_train)\n",
    "        feature_imp = pd.Series(model.feature_importances_,index=x.columns.values).sort_values(ascending=False)\n",
    "        sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "        # Add labels to your graph\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(\"Visualizing Important Features\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        logger.info('the important features have been selected as : {}'.format(selected_features))\n",
    "        # return the name of the important features\n",
    "        return selected_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of dataset\n",
    "df = pd.read_excel('nadataset.xlsx')\n",
    "# check for missing values in features\n",
    "# check information about df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sum of NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values and the percentage of na in columns\n",
    "feature_stat,features = FeatureEngineering.na_percentage_in_cols(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stat['na percentage'].hist(bins=10, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_stat,rows = FeatureEngineering.na_percentage_in_rows(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_stat['na percentage'].hist(bins=10, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature engineering >> handle missing, arranging features, outliers treatment\n",
    "df=FeatureEngineering.handle_missings(df)\n",
    "df=FeatureEngineering.rearrange_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataset\n",
    "df.iloc[:,0:3].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a plot for each feature\n",
    "# shows the values of feature and the coressponding number of observations \n",
    "font = {'size'   : 26}\n",
    "matplotlib.rc('font', **font)\n",
    "df.hist(bins=10, figsize=(40,40))\n",
    "plt.savefig('hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers detection\n",
    "font = {'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "fig, axs = plt.subplots(1,3, figsize = (12,3))\n",
    "plt1 = sns.boxplot(df['house_price'], ax = axs[0])\n",
    "plt2 = sns.boxplot(df['bedrooms'], ax = axs[1])\n",
    "plt3 = sns.boxplot(df['baths'], ax = axs[2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers treatment \n",
    "df = FeatureEngineering.drop_outliers(df,'house_price')\n",
    "\n",
    "font = {'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "fig, axs = plt.subplots(1,3, figsize = (12,3))\n",
    "plt1 = sns.boxplot(df['house_price'], ax = axs[0])\n",
    "plt2 = sns.boxplot(df['bedrooms'], ax = axs[1])\n",
    "plt3 = sns.boxplot(df['baths'], ax = axs[2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a plot for each feature\n",
    "# shows the values of feature and the coressponding number of observations \n",
    "font = {'size'   : 26}\n",
    "matplotlib.rc('font', **font)\n",
    "df.hist(bins=10, figsize=(40,40))\n",
    "plt.savefig('hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation between features as heatmap\n",
    "font = {'size'   : 10}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()\n",
    "plt.savefig('corr1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations to the rental price\n",
    "corr_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation features to the house price\n",
    "corr_matrix[corr_matrix.index == 'house_price'].T.sort_values('house_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the correlation as a heatmap\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the correlation more than 0.5 as a heatmap\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix[corr_matrix> 0.5], square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all fetures with correlation more than 0.3 with house _price\n",
    "t = corr_matrix[corr_matrix.index == 'house_price'].T.sort_values('house_price')\n",
    "t[t['house_price']>=0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[t['house_price']<=-0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of features that don't have correlation with house price\n",
    "t[t['house_price'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column with no correlation \n",
    "df1 = df.drop(columns=t[t['house_price'].isna()].index)\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.heatmap(df1.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()\n",
    "plt.savefig('corr2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['house_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2,2, figsize = (12,7))\n",
    "plt1 = sns.violinplot(df['bedrooms'], df['house_price'], ax = axs[0,0])\n",
    "plt2 = sns.violinplot(df['baths'], df['house_price'], ax = axs[1,0])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#plt.xticks(rotation=45)\n",
    "plt.title(\"Violin plot for bedrooms and baths to price\")\n",
    "plt.savefig('violinplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most important features \n",
    "# these features is related to the features correlation table\n",
    "selected_feat=FeatureSelection.select_features(df,'house_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataSplitter:\n",
    "    '''\n",
    "    Class dataSplitter to split data into train and test using selected features \n",
    "    '''\n",
    "    def __init__(self,df,target_variable,selected_feat):\n",
    "        #self.x = df.drop(target_variable, 1)\n",
    "        self.x = df[selected_feat]\n",
    "        self.y = df[target_variable]\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x,self.y,test_size=0.2,random_state=37)\n",
    "    \n",
    "    def scale_features(self):\n",
    "        '''\n",
    "        This function scale features usinf standardization \n",
    "        to have values of features between 1 and -1  \n",
    "        '''\n",
    "        pipeline = Pipeline([\n",
    "            ('std_scalar', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        # standardize x_train, x_test\n",
    "        self.x_train = pipeline.fit_transform(self.x_train)\n",
    "        self.x_test = pipeline.transform(self.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= dataSplitter(df,'house_price',selected_feat)\n",
    "data.scale_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot((data.x_test), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    '''\n",
    "    Class Model have all validation and evaluation methods with save method\n",
    "    '''\n",
    "    def cross_val(self,model,x,y):\n",
    "        '''\n",
    "        This function calculate cross_val_score for model\n",
    "        '''\n",
    "        cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "        # evaluate model\n",
    "        pred = cross_val_score(model, x, y,cv=cv)\n",
    "        logger.info('cross_val_score: {}'.format(pred.mean()))\n",
    "        return pred.mean()\n",
    "\n",
    "\n",
    "\n",
    "    def print_evaluate(self,true, predicted):  \n",
    "        '''\n",
    "        printing evaluation metrics\n",
    "        '''\n",
    "        mae, mse, rmse, r2_square=self.evaluate(true, predicted)\n",
    "        logger.info('MAE: {}'.format(mae))\n",
    "        logger.info('MSE: {}'.format(mse))\n",
    "        logger.info('RMSE: {}'.format(rmse))\n",
    "        logger.info('R2 Square: {}'.format(r2_square))\n",
    "        logger.info('__________________________________')\n",
    "        \n",
    "        print('MAE:', mae)\n",
    "        print('MSE:', mse)\n",
    "        print('RMSE:', rmse)\n",
    "        print('R2 Square', r2_square)\n",
    "        print('__________________________________')\n",
    "\n",
    "\n",
    "    def evaluate(self,true, predicted):\n",
    "        '''\n",
    "        calculate evaluation metrics for a model\n",
    "        '''\n",
    "        mae = mean_absolute_error(true, predicted)\n",
    "        mse = mean_squared_error(true, predicted)\n",
    "        rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "        r2_square = r2_score(true, predicted)\n",
    "        return mae, mse, rmse, r2_square   \n",
    "    \n",
    "    \n",
    "    def SaveModel(self,model,filename='finalized_model.sav'):\n",
    "        # Save the model as a pickle in a file\n",
    "        # save the model to disk\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "        \n",
    "        # some time later...\n",
    "        # load the model from disk\n",
    "    \n",
    "        #loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining(Model):\n",
    "    \n",
    "    '''\n",
    "    ModelTraining Class inhiret class Model to have evaluation metrics\n",
    "    this class have 6 model to train and hyperparameter tuning to get the best prediction model\n",
    "    '''\n",
    "    def __init__(self, data, dataTune):\n",
    "        '''Constructor'''\n",
    "        # results of all metrics for all models\n",
    "        self.results_df = pd.DataFrame()\n",
    "        # data to use in training \n",
    "        self.data = data\n",
    "        # data to use in hyper tuning \n",
    "        self.dataTune = dataTune\n",
    "        \n",
    "    def linearRegTrain(self):\n",
    "        ''' to train a linear regression model'''\n",
    "        logger.info('Training linear regression model')\n",
    "        # define object of linear regression model\n",
    "        model = LinearRegression(normalize=False)\n",
    "        # train the model\n",
    "        model.fit(self.data.x_train,self.data.y_train)\n",
    "        # predict \n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        \n",
    "        # recording metric evaluation \n",
    "        print('Test dataset evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train dataset evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        \n",
    "        # add all metrics data to a dataframe results_df\n",
    "        self.results_df = pd.DataFrame(data=[[\"Linear Regression\", *self.evaluate(self.data.y_test, test_pred) , self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "        # save the model \n",
    "        self.SaveModel(model,'LinReg.sav')\n",
    "        return model\n",
    "    \n",
    "\n",
    "##################### Random Forest Regressor#########################   \n",
    "    def RandomForestTrain(self):\n",
    "        ''' train a Random Forest Regressor model'''\n",
    "        logger.info('Training Random Forest Regressor model')\n",
    "        # get the best hypertuned model for Random Forest Regressor\n",
    "        model = self.HyperTuneRandomForest()\n",
    "        # train model\n",
    "        model.fit(self.data.x_train, self.data.y_train)\n",
    "        # predictions\n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        # recording evaluation metrics\n",
    "        print('Test set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        # add all metrics data to a dataframe results_df\n",
    "        results_df_1 = pd.DataFrame(data=[[\"Random Forest Regressor\", *self.evaluate(self.data.y_test, test_pred), self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "        self.results_df = self.results_df.append(results_df_1, ignore_index=True)\n",
    "        #save model\n",
    "        self.SaveModel(model,'RandForest.sav')\n",
    "        return model\n",
    "    \n",
    "    def HyperTuneRandomForest(self):\n",
    "        '''Random Forest Regressor hyper parameters tuning'''\n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap}\n",
    "        # defin object of RandomForestRegressor to hyper tune the parameter\n",
    "        rf = RandomForestRegressor()\n",
    "        # Random search of parameters, using 3 fold cross validation, \n",
    "        # search across 100 different combinations, and use all available cores\n",
    "        rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "        # Fit the random search model\n",
    "        rf_random.fit(self.dataTune.x_train, self.dataTune.y_train)\n",
    "        logger.info('best param for: {}'.format(rf_random.best_params_))\n",
    "        return rf_random.best_estimator_\n",
    "    \n",
    "###################### Gradient Boosting Regressor ####################\n",
    "\n",
    "    def GradientBoostingTrain(self):\n",
    "        '''Training Gradient Boosting Regressor model'''\n",
    "        logger.info('Training Gradient Boosting Regressor model')\n",
    "        # get the best model of Gradient Boosting Regressor after hyper parameters tuning\n",
    "        model = self.HyperTuneGradientBoosting()\n",
    "        # train the model\n",
    "        model.fit(self.data.x_train, self.data.y_train)\n",
    "        # prediction\n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        # recording evaluation metrics\n",
    "        print('Test dataset evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train dataset evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        # add all metrics data to a dataframe results_df\n",
    "        results_df_1 = pd.DataFrame(data=[[\"Gradient Boosting Regressor\", *self.evaluate(self.data.y_test, test_pred), self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "        self.results_df = self.results_df.append(results_df_1, ignore_index=True)\n",
    "        # save model\n",
    "        self.SaveModel(model,'GradBoost.sav')\n",
    "        return model\n",
    "    \n",
    "    def HyperTuneGradientBoosting(self):\n",
    "        '''Gradient Boosting Regressor hyper parameters tuning'''\n",
    "        # hyper parameters\n",
    "        params = {'n_estimators':[500, 1000, 1500, 2000], 'max_depth':[3, 5, 8],'random_state':[22,37,50]}\n",
    "        # define object of GradientBoostingRegressor model\n",
    "        gbr = GradientBoostingRegressor()\n",
    "        # create GridSearchCV object to search for the best estimator\n",
    "        gbr_grid = GridSearchCV(gbr, params, cv=5)\n",
    "        gbr_grid.fit(self.dataTune.x_train, self.dataTune.y_train)\n",
    "        logger.info('best param for: {}'.format(gbr_grid.best_params_))\n",
    "        return gbr_grid.best_estimator_\n",
    "    \n",
    "#################### ElasticNet ####################\n",
    "\n",
    "    def ElasticNetTrain(self):\n",
    "        '''Training ElasticNet model'''\n",
    "        logger.info('Training ElasticNet model')\n",
    "        # hyperparameters to be tuned\n",
    "        elastic_params = {'alpha':np.arange(0, 1, 0.01)}\n",
    "        # hyperparameters tuning using GridSearchCV\n",
    "        best_estim = GridSearchCV(ElasticNet(), param_grid=elastic_params).fit(self.dataTune.x_train, self.dataTune.y_train).best_estimator_\n",
    "        # get the best model of ElasticNet after hyperparameters tuning using GridSearchCV\n",
    "        model = best_estim\n",
    "        # train the model\n",
    "        model.fit(self.data.x_train, self.data.y_train)\n",
    "\n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        # recording evaluation metrics\n",
    "        print('Test set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        # add all metrics data to a dataframe results_df\n",
    "        results_df_1 = pd.DataFrame(data=[[\"ElasticNet Regressor\", *self.evaluate(self.data.y_test, test_pred), self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "        self.results_df = self.results_df.append(results_df_1, ignore_index=True)\n",
    "        #save model\n",
    "        self.SaveModel(model,'Elastic.sav')\n",
    "        return model\n",
    "\n",
    "##################### Lasso ####################\n",
    "    def LassoTrain(self): \n",
    "        '''Training Lasso model'''\n",
    "        logger.info('Training Lasso model')\n",
    "        # hyperparameters to be tuned\n",
    "        lasso_params = {'alpha':np.arange(0, 1, 0.01)}\n",
    "        # hyperparameters tuning using GridSearchCV\n",
    "        best_estim = GridSearchCV(Lasso(), param_grid=lasso_params).fit(self.dataTune.x_train, self.dataTune.y_train).best_estimator_\n",
    "        # get the best model of Lasso after hyperparameters tuning using GridSearchCV\n",
    "        model = best_estim\n",
    "        # train the model\n",
    "        model.fit(self.data.x_train, self.data.y_train)\n",
    "\n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        # recording evaluation metrics\n",
    "        print('Test set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        # add all metrics data to a dataframe results_df\n",
    "        results_df_1 = pd.DataFrame(data=[[\"Lasso Regressor\", *self.evaluate(self.data.y_test, test_pred), self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "        self.results_df = self.results_df.append(results_df_1, ignore_index=True)\n",
    "        # save model\n",
    "        self.SaveModel(model,'Lasso.sav')\n",
    "        return model\n",
    "    \n",
    "##################### Ridge ####################\n",
    "    def RidgeTrain(self):\n",
    "        '''Training Ridge model'''\n",
    "        logger.info('Training Ridge model')\n",
    "        # hyperparameters to be tuned\n",
    "        ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}\n",
    "        # hyperparameters tuning using GridSearchCV\n",
    "        best_estim = GridSearchCV(Ridge(), param_grid=ridge_params).fit(self.dataTune.x_train, self.dataTune.y_train).best_estimator_\n",
    "        # get the best model of Ridge after hyperparameters tuning using GridSearchCV\n",
    "        model = best_estim\n",
    "        # train model\n",
    "        model.fit(self.data.x_train, self.data.y_train)\n",
    "\n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        # recording evaluation metrics\n",
    "        print('Test set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        # add all metrics data to a dataframe results_df\n",
    "        results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *self.evaluate(self.data.y_test, test_pred), self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                                    columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "        self.results_df = self.results_df.append(results_df_2, ignore_index=True)\n",
    "        # save model\n",
    "        self.SaveModel(model,'Ridge.sav')\n",
    "        return model\n",
    "\n",
    "##################### KNN ####################\n",
    "    def KNNTrain(self):\n",
    "        '''Training KNeighbors Regressor model'''\n",
    "        logger.info('Training KNeighbors Regressor model')\n",
    "        # Define hyperparameters\n",
    "        hp_params = {'n_neighbors': [100,200,300], 'weights': ['uniform','distance']}\n",
    "        \n",
    "        # Search for best hyperparameters and get the best estimator\n",
    "        knr = GridSearchCV(estimator= KNeighborsRegressor(), param_grid=hp_params, scoring='r2').fit(self.dataTune.x_train, self.dataTune.y_train).best_estimator_\n",
    "        # get the best model of Ridge after hyperparameters tuning using GridSearchCV\n",
    "        model = knr\n",
    "        # ttrain model\n",
    "        model.fit(self.data.x_train, self.data.y_train)\n",
    "        \n",
    "        test_pred = model.predict(self.data.x_test)\n",
    "        train_pred = model.predict(self.data.x_train)\n",
    "        # recording evaluation metrics\n",
    "        print('Test set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_test, test_pred)\n",
    "        print('Train set evaluation:\\n_____________________________________')\n",
    "        self.print_evaluate(self.data.y_train, train_pred)\n",
    "        # add all metrics data to a dataframe results_df\n",
    "        results_df_1 = pd.DataFrame(data=[[\"KNeighbors Regressor\", *self.evaluate(self.data.y_test, test_pred), self.cross_val(model,self.data.x_test,self.data.y_test)]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "        self.results_df = self.results_df.append(results_df_1, ignore_index=True)\n",
    "        # save model\n",
    "        self.SaveModel(model,'KNR.sav')\n",
    "        return model   \n",
    "#######################################################    \n",
    "    def predict(self,model,x,y):\n",
    "        '''function to use trained model to do predictions'''\n",
    "        pred_y = model.predict(x)\n",
    "        self.print_evaluate(y,pred_y)\n",
    "        df = pd.DataFrame()\n",
    "        df['True'] = y.tolist()\n",
    "        df['Prediction'] = pred_y.tolist()\n",
    "        logger.info('Prediction: {}'.format(df))\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for tuning \n",
    "d = pd.read_excel('data.xlsx')\n",
    "d=d.drop(columns=['Unnamed: 0'])\n",
    "d=d.astype('int')\n",
    "d=FeatureEngineering.drop_outliers(d,'house_price')\n",
    "d=FeatureEngineering.drop_outliers(d,'bedrooms')\n",
    "d=FeatureEngineering.drop_outliers(d,'baths')\n",
    "dataTune = dataSplitter(d,'house_price',selected_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features \n",
    "dataTune.scale_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for training \n",
    "data = dataSplitter(df,'house_price',selected_feat)\n",
    "# scale features\n",
    "data.scale_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define object of ModelTraining\n",
    "training = ModelTraining(data,dataTune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Linear Regressor\n",
    "training.linearRegTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train Gradient Boosting Regressor\n",
    "training.GradientBoostingTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest Regressor\n",
    "training.RandomForestTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ElasticNet Regressor\n",
    "training.ElasticNetTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Lasso Regressor\n",
    "training.LassoTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Ridge Regressor\n",
    "training.RidgeTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knieghbors Regressor\n",
    "training.KNNTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results of metrics for each model #R2 Square\n",
    "training.results_df.set_index('Model', inplace=True)\n",
    "training.results_df['R2 Square'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model to use \n",
    "#training.results_df.sort_values(by=['R2 Square']).iloc[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data  \n",
    "d = pd.read_excel('cleanDataSet.xlsx')\n",
    "d=d.drop(columns=['Unnamed: 0'])\n",
    "d=d.astype('int')\n",
    "d=FeatureEngineering.drop_outliers(d,'house_price')\n",
    "d=FeatureEngineering.drop_outliers(d,'bedrooms')\n",
    "d=FeatureEngineering.drop_outliers(d,'baths')\n",
    "d = d.sample(10)\n",
    "dat = dataSplitter(d,'house_price',selected_feat)\n",
    "# scale features \n",
    "dat.scale_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"Elastic.sav\", \"KNR.sav\", \"Lasso.sav\", \"LinReg.sav\", \"RandForest.sav\", \"Ridge.sav\", \"GrandBoost.sav\" ]:\n",
    "    print(model)\n",
    "    loaded_model = pickle.load(open(model, 'rb'))\n",
    "    print(training.predict(loaded_model,dat.x_test,dat.y_test))\n",
    "    print('<><><><><><><><><><><><><><><><><><><><><><><><>')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
